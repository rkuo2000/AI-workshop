---
layout: post
title: Large Language Models
author: [Richard Kuo]
category: [Lecture]
tags: [jekyll, ai]
---

Introduction to Large Language Models.

---
## History of LLM
Since the introduction of Transformer model in 2017, large language models (LLMs) have evolved significantly. ChatGPT saw 1.6B visits in May 2023. Meta also released three versions of LLaMA-2 (7B, 13B, 70B) free for commercial use in July.

### LLM Landscape
![](https://www.johnsnowlabs.com/wp-content/uploads/2023/07/1_3Q0RKSk_LUpGuBjxJ4C-Wg.png)
![](https://substackcdn.com/image/fetch/w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F2e8bfd65-5272-4cf1-8b86-954bab975bab_2400x1350.png)

---
### Companies Landscape
![](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*vZK250i8PIWid6BiaZ1QCA.png)

---
### Growth of Compute Memory vs. Transformer Size
Ref. [AI and Memory Wall](https://medium.com/riselab/ai-and-memory-wall-2cb4265cb0b8)<br>
![](https://miro.medium.com/v2/resize:fit:4800/format:webp/0*U-7GJqBZ2tY1W5Iu)

---
![](https://www.insightpartners.com/wp-content/uploads/2023/10/llmops-market-map-1.png)

---
## Transformer
**Paper:** [Attention Is All You Need](https://arxiv.org/abs/1706.03762)<br>
**Code:** [huggingface/transformers](https://github.com/huggingface/transformers)<br>
![](https://miro.medium.com/max/407/1*3pxDWM3c1R_WSW7hVKoaRA.png)
<table>
<tr>
<td><iframe width="400" height="300" src="https://www.youtube.com/embed/n9TlOhRjYoc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
<td><iframe width="400" height="300" src="https://www.youtube.com/embed/N6aRv06iv2g" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td>
</tr>
</table>

---
### New Understanding about Transformer
**Blog:** <br>
* [Researchers Gain New Understanding From Simple AI](https://www.quantamagazine.org/researchers-glimpse-how-ai-gets-so-good-at-language-processing-20220414/)
* [Transformer稱霸的原因找到了？OpenAI前核心員工揭開注意力頭協同工作機理](https://bangqu.com/A76oX7.html)

**Papers:**<br>
* [A Mathematical Framework for Transformer Circuits](https://transformer-circuits.pub/2021/framework/index.html)
* [In-context Learning and Induction Heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)

---
### BERT
**Paper:** [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)<br>
**Blog:** [進擊的BERT：NLP 界的巨人之力與遷移學習](https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html)<br>

---
### GPT (Generative Pre-Training Transformer)
**Paper:** [Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)<br>
**Paper:** [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)<br>
<iframe width="640" height="480" src="https://www.youtube.com/embed/WY_E0Sd4K80" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
**Code:** [https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)<br>

### GPT-2
**Paper:** [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)<br>
**Code:** [openai/gpt-2](https://github.com/openai/gpt-2)<br>
**GPT2 Demo:** [Transformer Demo](https://app.inferkit.com/demo), [GPT-2 small](https://minimaxir.com/apps/gpt2-small/)<br>
**Blog:** [直觀理解GPT2語言模型並生成金庸武俠小說](https://leemeng.tw/gpt2-language-model-generate-chinese-jing-yong-novels.html)<br>

---
### T5: Text-To-Text Transfer Transformer (by Google)
**Paper:** [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)<br>
**Code:** [google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer)<br>
![](https://1.bp.blogspot.com/-o4oiOExxq1s/Xk26XPC3haI/AAAAAAAAFU8/NBlvOWB84L0PTYy9TzZBaLf6fwPGJTR0QCLcBGAsYHQ/s640/image3.gif)
![](https://1.bp.blogspot.com/-89OY3FjN0N0/XlQl4PEYGsI/AAAAAAAAFW4/knj8HFuo48cUFlwCHuU5feQ7yxfsewcAwCLcBGAsYHQ/s640/image2.png)

---
### GPT-3
**Code:** [openai/gpt-3](https://github.com/openai/gpt-3)<br>
**[GPT-3 Demo](https://gpt3demo.com/)**<br>
![](https://dzlab.github.io/assets/2020/07/20200725-gpt3-model-architecture.png)

---
### [CKIP Lab 繁體中文詞庫小組](https://ckip.iis.sinica.edu.tw/)
CKIP (CHINESE KNOWLEDGE AND INFORMATION PROCESSING): 繁體中文的 transformers 模型（包含 ALBERT、BERT、GPT2）及自然語言處理工具。<br>
[CKIP Lab 下載軟體與資源](https://ckip.iis.sinica.edu.tw/resource)<br>
* [CKIP Transformers](https://github.com/ckiplab/ckip-transformers)
* [CKIP Tagger](https://github.com/ckiplab/ckiptagger)<br>

---
## Question Answering
### [SQuAD 2.0](https://rajpurkar.github.io/SQuAD-explorer/) - The Stanford Question Answering Dataset<br>
**Paper:** [Know What You Don't Know: Unanswerable Questions for SQuAD](https://arxiv.org/abs/1806.03822)<br>
![](https://miro.medium.com/max/1400/1*Tqibs5z0zCntcK6kCpziaA.png)

---
### Instruct GPT
**Paper:** [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)<br>
**Blog:** [Aligning Language Models to Follow Instructions](https://openai.com/blog/instruction-following/)<br>

---
### ChatGPT
[ChatGPT: Optimizing Language Models for Dialogue](https://openai.com/blog/chatgpt/)<br>
ChatGPT is fine-tuned from a model in the GPT-3.5 series, which finished training in early 2022.<br>

![](https://cdn.openai.com/chatgpt/draft-20221129c/ChatGPT_Diagram.svg)

<iframe width="640" height="455" src="https://www.youtube.com/embed/e0aKI2GGZNg" title="Chat GPT (可能)是怎麼煉成的 - GPT 社會化的過程" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

---
### GPT4
**Paper:** [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774)<br>
![](https://image-cdn.learnin.tw/bnextmedia/image/album/2023-03/img-1679884936-23656.png?w=1200&output=webp)

---
### MiniGPT-4
**Paper:** [MiniGPT-4: Enhancing Vision-language Understanding with Advanced Large Language Models](https://arxiv.org/abs/2304.10592<br>
**Paper:** [MiniGPT-v2: Large Language Model as a Unified Interface for Vision-Language Multi-task Learning](https://arxiv.org/abs/2310.09478)<br>
**Code:** [https://github.com/Vision-CAIR/MiniGPT-4](https://github.com/Vision-CAIR/MiniGPT-4)<br>

![](https://github.com/Vision-CAIR/MiniGPT-4/raw/main/figs/minigpt2_demo.png(https://github.com/Vision-CAIR/MiniGPT-4/raw/main/figs/minigpt2_demo.png)
![](https://github.com/Vision-CAIR/MiniGPT-4/raw/main/figs/online_demo.png)


---
### [ALTER-LLM](https://tnoinkwms.github.io/ALTER-LLM/)
**Paper:** [From Text to Motion: Grounding GPT-4 in a Humanoid Robot "Alter3"]()<br>
<iframe width="593" height="346" src="https://www.youtube.com/embed/SAc-O5FDJ4k" title="play the metal" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
![](https://tnoinkwms.github.io/ALTER-LLM/architecture_2.png)
![](https://tnoinkwms.github.io/ALTER-LLM/feedback.png)

<br>
<br>

*This site was last updated {{ site.time | date: "%B %d, %Y" }}.*

